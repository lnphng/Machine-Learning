{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune XGBoost model\n",
    "\n",
    "- make `XGBoost` models as performant as possible. \n",
    "- learn about the variety of parameters that can be adjusted to alter the behavior of `XGBoost`\n",
    "- and how to tune them efficiently so that supercharge the performance of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ames_housing_trimmed_processed.csv')\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.300782\n",
      "1                   10  34774.192709\n",
      "2                   15  32895.096354\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> increasing the number of boosting rounds decreases the RMSE.\n",
    "\n",
    "### Automated boosting round selection using early_stopping\n",
    "- use XGBoost automatically select the number of boosting rounds within xgb.cv(). This is done using a technique called `early stopping`.\n",
    "- Early stopping works by:\n",
    "    - testing the XGBoost model after every boosting round against a hold-out dataset \n",
    "    - and stopping the creation of additional boosting rounds (thereby finishing training of the model early) \n",
    "        - if the hold-out metric (ex \"rmse\") does not improve for a given number of rounds. \n",
    "        - Ex: use the `early_stopping_rounds` parameter in xgb.cv() with a large possible number of boosting rounds (50).\n",
    "        - if the holdout metric continuously improves up through when `num_boost_rounds` is reached, then early stopping does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.630208      403.632409   142640.645833     705.565658\n",
      "1     103057.031250       73.772931   104907.666667     111.114933\n",
      "2      75975.963542      253.734987    79262.059896     563.766991\n",
      "3      57420.528646      521.655155    61620.135417    1087.690754\n",
      "4      44552.955729      544.169200    50437.562500    1846.448017\n",
      "5      35763.950521      681.797429    43035.660156    2034.469858\n",
      "6      29861.464844      769.570645    38600.881511    2169.800969\n",
      "7      25994.673177      756.520694    36071.817708    2109.795430\n",
      "8      23306.833333      759.237086    34383.184896    1934.546688\n",
      "9      21459.768880      745.624404    33509.141927    1887.377720\n",
      "10     20148.720703      749.612103    32916.806641    1850.893136\n",
      "11     19215.382161      641.388291    32197.834635    1734.458508\n",
      "12     18627.388021      716.257152    31770.853516    1802.155409\n",
      "13     17960.695312      557.043498    31482.782552    1779.123767\n",
      "14     17559.736979      631.412969    31389.990234    1892.319927\n",
      "15     17205.713867      590.172770    31302.882162    1955.165902\n",
      "16     16876.570964      703.631690    31234.058594    1880.705796\n",
      "17     16597.662760      703.676764    31318.348958    1828.861483\n",
      "18     16330.461589      607.273497    31323.634115    1775.908526\n",
      "19     16005.973307      520.471073    31204.136068    1739.077074\n",
      "20     15814.301432      518.604477    31089.862630    1756.020157\n",
      "21     15493.405924      505.616658    31047.996094    1624.673955\n",
      "22     15270.734049      502.019652    31056.916016    1668.042813\n",
      "23     15086.382487      503.912747    31024.985026    1548.985856\n",
      "24     14917.607747      486.206362    30983.684896    1663.128699\n",
      "25     14709.588868      449.666844    30989.475261    1686.666560\n",
      "26     14457.286458      376.787206    30952.113281    1613.172049\n",
      "27     14185.566406      383.102876    31066.902344    1648.534310\n",
      "28     13934.067383      473.465256    31095.639974    1709.224745\n",
      "29     13749.646159      473.671021    31103.888021    1778.879153\n",
      "30     13549.836914      454.898923    30976.086589    1744.515448\n",
      "31     13413.484375      399.603774    30938.469401    1746.052597\n",
      "32     13275.915364      415.408340    30931.000651    1772.468466\n",
      "33     13085.878255      493.792778    30929.055990    1765.539740\n",
      "34     12947.181315      517.790039    30890.631510    1786.509640\n",
      "35     12846.027669      547.732453    30884.494141    1769.728223\n",
      "36     12702.378581      505.523563    30833.542318    1691.002985\n",
      "37     12532.244466      508.298083    30856.688802    1771.446777\n",
      "38     12384.054362      536.225108    30818.016927    1782.784718\n",
      "39     12198.443685      545.165137    30839.391927    1847.327847\n",
      "40     12054.583333      508.841412    30776.966146    1912.781920\n",
      "41     11897.036784      477.177622    30794.701172    1919.675316\n",
      "42     11756.221680      502.992419    30780.954427    1906.819550\n",
      "43     11618.846029      519.838297    30783.755859    1951.259316\n",
      "44     11484.080404      578.428621    30776.731120    1953.446310\n",
      "45     11356.552734      565.368794    30758.543620    1947.456345\n",
      "46     11193.557292      552.298848    30729.971354    1985.698867\n",
      "47     11071.315755      604.089261    30732.664062    1966.998275\n",
      "48     10950.778320      574.862779    30712.240886    1957.751118\n",
      "49     10824.865560      576.666458    30720.854818    1950.511520\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(params= params, \n",
    "                    dtrain=housing_dmatrix, \n",
    "                    nfold=3, metrics ='rmse',\n",
    "                    seed=123,\n",
    "                    num_boost_round=50,\n",
    "                    early_stopping_rounds=10, \n",
    "                    as_pandas=True)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning eta\n",
    "- tuning the \"eta\", also known as the learning rate.\n",
    "- The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.401042\n",
      "1  0.010  179932.182292\n",
      "2  0.100   79759.414063\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=10, early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth\n",
    "- tuning max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.599610\n",
      "2         10  36065.546875\n",
      "3         20  36739.578125\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\", 'max_depth':20}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2,5,10,20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                        params=params,\n",
    "                        metrics='rmse',\n",
    "                        seed=123, \n",
    "                        num_boost_round=10,\n",
    "                        nfold=2,\n",
    "                        early_stopping_rounds=5 ,\n",
    "                        as_pandas=True)\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree\n",
    "- tuning `colsample_bytree`. \n",
    "- With `scikit-learn`'s `RandomForestClassifier` or `RandomForestRegressor`, where it just was called `max_features`. \n",
    "- In both `xgboost` and `sklearn`, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, `colsample_bytree` must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  40918.115235\n",
      "1               0.5  35813.906250\n",
      "2               0.8  35995.679688\n",
      "3               1.0  35836.044922\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1,0.5,0.8,1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of grid search and random search\n",
    "\n",
    "#### Grid search with XGBoost\n",
    "- take parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. \n",
    "- use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  28986.18703093561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm,\n",
    "                        param_grid= gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=4,\n",
    "                        verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search with XGBoost\n",
    "- `GridSearchCV` can be really time consuming, \n",
    "- so in practice, may use `RandomizedSearchCV` instead\n",
    "- the key difference is to specify a `param_distributions` parameter instead of a `param_grid` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  29998.4522530019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    5.8s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid,                                           \n",
    "                                    estimator=gbm,\n",
    "                                    scoring= 'neg_mean_squared_error',\n",
    "                                    n_iter=5,\n",
    "                                    cv=4,\n",
    "                                    verbose =1)\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
