{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing in Python\n",
    "- learn natural language processing (NLP) basics, such as \n",
    "    - how to identify and separate words, how to extract topics in a text, and \n",
    "    - how to build your own fake news classifier. \n",
    "- learn how to use basic libraries such as\n",
    "    - NLTK, alongside libraries which utilize deep learning to solve common NLP problems. \n",
    "- This course will give you the foundation to process and parse text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions & word tokenization\n",
    "- learn some basic NLP concepts, such as word tokenization and regular expressions to help parse text. \n",
    "- also learn how to handle non-English text and more difficult tokenization you might find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The sentences: [\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', ''] \n",
      "\n",
      "2. All capitalized words: ['Let', 'RegEx', 'Won', 'Can', 'Or'] \n",
      "\n",
      "3. All words: [\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?'] \n",
      "\n",
      "4. All digits: ['4', '19'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print('1. The sentences:',re.split(sentence_endings, my_string),'\\n')\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print('2. All capitalized words:',re.findall(capitalized_words, my_string),'\\n')\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print('3. All words:',re.split(spaces, my_string), '\\n')\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print('4. All digits:',re.findall(digits, my_string),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#scene_one= pd.read_csv('grail.txt',sep='\\t',skiprows=1, low_memory=False)\n",
    "\n",
    "# Open a file: file\n",
    "file = open('grail.txt',mode='r')\n",
    " \n",
    "# read all lines at once\n",
    "scene_one = file.read()\n",
    " \n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!\n",
      "[clop clop clop] \n",
      "SOLDIER #1: Halt!\n",
      "Who goes there?\n",
      "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.\n",
      "King of the Britons, defeator of the Saxons, sovereign of all England!\n",
      "SOLDIER #1: Pull the other one!\n",
      "ARTHUR: I am, ...  and this is my trusty servant Patsy.\n",
      "We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.\n",
      "I must speak with your lord and master.\n",
      "SOLDIER #1: What?\n"
     ]
    }
   ],
   "source": [
    "for i in sentences[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTHUR',\n",
       " ':',\n",
       " 'It',\n",
       " 'is',\n",
       " 'I',\n",
       " ',',\n",
       " 'Arthur',\n",
       " ',',\n",
       " 'son',\n",
       " 'of',\n",
       " 'Uther',\n",
       " 'Pendragon',\n",
       " ',',\n",
       " 'from',\n",
       " 'the',\n",
       " 'castle',\n",
       " 'of',\n",
       " 'Camelot',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carry', 'known', 'hacked', 'writing', 'vicious', 'Walk', 'ride', 'bois', 'bottoms']\n"
     ]
    }
   ],
   "source": [
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "count = 0\n",
    "unique_tokens_list=[]\n",
    "for i in unique_tokens:\n",
    "    count = count + 1\n",
    "    if count == 10:\n",
    "        break\n",
    "    unique_tokens_list.append(i)\n",
    "print(unique_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Use re.search to find the first text in square brackets\n",
    "pattern1 = r'\\[.*\\]'\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[A-Z+]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This is the best #nlp exercise I\\'ve found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([\\@\\#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[(len(tweets)-1)], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', \"I've\", 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? bb Und fährst du mit Über? 🚕'\n",
    "german_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imported from `nltk.tokenize`: `regexp_tokenize` and `word_tokenize`.\n",
    "- Unicode ranges for emoji are:\n",
    "\n",
    "    - `('\\U0001F300'-'\\U0001F5FF')`, \n",
    "    - `('\\U0001F600-\\U0001F64F')`, \n",
    "    - `('\\U0001F680-\\U0001F6FF')`, and\n",
    "    - `('\\u2600'-\\u26FF-\\u2700-\\u27BF')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1. All capitalized words: ['Wann', 'Pizza', 'Und', 'Über'] \n",
      "\n",
      "C2. All capitalized words: ['Wann', 'Pizza', 'Und', 'Über']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print('C1. All capitalized words:', regexp_tokenize(german_text, capital_words),'\\n')\n",
    "print('C2. All capitalized words:',re.b(capital_words, german_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!',\n",
       " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " 'SOLDIER #1: What?  Ridden on a horse?',\n",
       " 'ARTHUR: Yes!',\n",
       " \"SOLDIER #1: You're using coconuts!\",\n",
       " 'ARTHUR: What?']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holy_grail = scene_one\n",
    "lines = holy_grail.split('\\n')\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!',\n",
       " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " 'SOLDIER #1: What?  Ridden on a horse?',\n",
       " 'ARTHUR: Yes!',\n",
       " \"SOLDIER #1: You're using coconuts!\",\n",
       " 'ARTHUR: What?']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "lines[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " ' Whoa there!  [clop clop clop] ',\n",
       " ' Halt!  Who goes there?',\n",
       " ' It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " ' Pull the other one!',\n",
       " ' I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " ' What?  Ridden on a horse?',\n",
       " ' Yes!',\n",
       " \" You're using coconuts!\",\n",
       " ' What?']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 4, 25, 4, 40, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s,r\"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "line_num_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(line_num_words, bins=(25))\n",
    "\n",
    "# Show bthe plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
